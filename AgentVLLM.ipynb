{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101cff18-967d-4844-a4fc-1f2bcc370a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sviatoslav/anaconda3/envs/vllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, Model, Tool, ChatMessage\n",
    "from smolagents.models import remove_stop_sequences\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf0f339-8e5c-408a-b018-ce8274d8872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 15:38:37 __init__.py:207] Automatically detected platform cpu.\n",
      "INFO 02-24 15:38:44 config.py:560] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 02-24 15:38:44 arg_utils.py:1223] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 02-24 15:38:44 config.py:696] Async output processing is not supported on the current platform type cpu.\n",
      "WARNING 02-24 15:38:44 cpu.py:63] CUDA graph is not supported on CPU, fallback to the eager mode.\n",
      "WARNING 02-24 15:38:44 cpu.py:78] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\n",
      "WARNING 02-24 15:38:44 cpu.py:99] uni is not supported on CPU, fallback to mp distributed executor backend.\n",
      "INFO 02-24 15:38:44 importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "INFO 02-24 15:38:44 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.4.dev53+geb24dc4a.d20250223+cpu) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-24 15:38:46 cpu.py:40] Using Torch SDPA backend.\n",
      "INFO 02-24 15:38:46 parallel_state.py:948] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 02-24 15:38:47 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 02-24 15:38:47 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 15:38:48 executor_base.py:111] # cpu blocks: 8192, # CPU blocks: 0\n",
      "INFO 02-24 15:38:48 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 1.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 15:38:48 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 0.54 seconds\n"
     ]
    }
   ],
   "source": [
    "class VLLMModel(Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        sampling_kwargs: dict = None,\n",
    "        init_kwargs: dict = None,\n",
    "        chat_kw_args: dict = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        default_model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "        if model_id is None:\n",
    "            model_id = default_model_id\n",
    "            logger.warning(f\"`model_id`not provided, using this default model: '{model_id}'\")\n",
    "        self.model_id = model_id\n",
    "        from vllm import LLM, SamplingParams\n",
    "        if not init_kwargs:\n",
    "            init_kwargs = {}\n",
    "        if not sampling_kwargs:\n",
    "            sampling_kwargs = {}\n",
    "        default_max_tokens = 5000\n",
    "        max_new_tokens = sampling_kwargs.get(\"max_new_tokens\") or sampling_kwargs.get(\"max_tokens\")\n",
    "        if not max_new_tokens:\n",
    "            kwargs[\"max_new_tokens\"] = default_max_tokens\n",
    "            logger.warning(\n",
    "                f\"`max_new_tokens` not provided, using this default value for `max_new_tokens`: {default_max_tokens}\"\n",
    "            )\n",
    "        self.kwargs = kwargs\n",
    "        self.sampling_params = SamplingParams(**sampling_kwargs)\n",
    "        self.model = LLM(model=model_name, **init_kwargs)\n",
    "        self._is_vlm = False\n",
    "\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        stop_sequences: Optional[List[str]] = None,\n",
    "        grammar: Optional[str] = None,\n",
    "        tools_to_call_from: Optional[List[Tool]] = None,\n",
    "        images: Optional[List[Image.Image]] = None,\n",
    "        **kwargs,\n",
    "    ) -> ChatMessage:\n",
    "        max_new_tokens = (\n",
    "            kwargs.get(\"max_new_tokens\")\n",
    "            or kwargs.get(\"max_tokens\")\n",
    "            or self.kwargs.get(\"max_new_tokens\")\n",
    "            or self.kwargs.get(\"max_tokens\")\n",
    "        )\n",
    "        completion_kwargs = {}\n",
    "        if max_new_tokens:\n",
    "            completion_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        import torch\n",
    "\n",
    "        out = self.model.chat(messages, \n",
    "                              sampling_params=self.sampling_params,\n",
    "                              use_tqdm=False)\n",
    "        output = out[-1].outputs[-1].text\n",
    "        if stop_sequences is not None:\n",
    "            output = remove_stop_sequences(output, stop_sequences)\n",
    "        raw = {'output': torch.tensor(out[-1].outputs[-1].token_ids), \n",
    "               \"completion_kwargs\": completion_kwargs}\n",
    "        if tools_to_call_from is None:\n",
    "            return ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=output,\n",
    "                raw=raw,\n",
    "            )\n",
    "        else:\n",
    "            if \"Action:\" in output:\n",
    "                output = output.split(\"Action:\", 1)[1].strip()\n",
    "            try:\n",
    "                start_index = output.index(\"{\")\n",
    "                end_index = output.rindex(\"}\")\n",
    "                output = output[start_index : end_index + 1]\n",
    "            except Exception as e:\n",
    "                raise Exception(\"No json blob found in output!\") from e\n",
    "\n",
    "            try:\n",
    "                parsed_output = json.loads(output)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Tool call '{output}' has an invalid JSON structure: {e}\")\n",
    "            tool_name = parsed_output.get(\"name\")\n",
    "            tool_arguments = parsed_output.get(\"arguments\")\n",
    "            return ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=\"\",\n",
    "                tool_calls=[\n",
    "                    ChatMessageToolCall(\n",
    "                        id=\"\".join(random.choices(\"0123456789\", k=5)),\n",
    "                        type=\"function\",\n",
    "                        function=ChatMessageToolCallDefinition(name=tool_name, arguments=tool_arguments),\n",
    "                    )\n",
    "                ],\n",
    "                raw=raw,\n",
    "            )\n",
    "\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "#model_name = \"facebook/opt-125m\"\n",
    "model = VLLMModel(model_name, sampling_kwargs={'max_tokens': 6000, 'temperature':0.8, 'top_p':0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964a5946-3c68-424f-aadb-d6d093893397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 15:38:49 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "WARNING 02-24 15:38:52 cpu.py:143] Pin memory is not supported on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatMessage(role='assistant', content=\"The Importance of Higher Education: Unlocking the Potential of the Next Generation\\n\\nHigher education has long been regarded as the key to unlocking the potential of individuals, societies, and economies. It is a cornerstone of modern civilization, providing individuals with the skills, knowledge, and expertise necessary to succeed in an increasingly complex and interconnected world. The importance of higher education cannot be overstated, and it is essential for fostering a more informed, productive, and prosperous society.\\n\\nOne of the primary reasons higher education is so crucial is that it equips individuals with the knowledge and skills necessary to compete in the modern job market. In today's global economy, the standard of living and economic growth are directly tied to the level of education and training. Higher education institutions provide students with access to a vast array of courses, programs, and research opportunities that can help them develop the skills and expertise required to succeed in their chosen field. For instance, studies have shown that individuals with higher levels of education tend to earn higher salaries, enjoy better job prospects, and experience reduced poverty rates.\\n\\nFurthermore, higher education plays a critical role in fostering a culture of innovation and creativity. By providing access to cutting-edge research and academic resources, universities and higher education institutions can stimulate the development of new ideas, technologies, and products. This, in turn, can lead to economic growth, job creation, and improved living standards. For example, many countries that have invested heavily in higher education have experienced rapid economic growth and innovation, as seen in the case of Singapore, which has become a hub for cutting-edge research and technology.\\n\\nHigher education also plays a vital role in addressing social and economic inequalities. By providing access to higher education, governments and institutions can help bridge the gap between the haves and have-nots, ensuring that everyone has the opportunity to develop the skills and knowledge necessary to participate fully in society. This, in turn, can help to reduce poverty, inequality, and social exclusion.\\n\\nAnother critical aspect of higher education is its ability to foster critical thinking, problem-solving, and collaboration skills. By encouraging students to think critically, explore complex issues, and work in teams, universities and higher education institutions can help to develop the skills necessary to tackle the complex problems facing our world today. This, in turn, can lead to improved decision-making, policy development, and community engagement.\\n\\nIn addition, higher education has a profound impact on social mobility. By providing opportunities for individuals to pursue higher education, universities and institutions can help to break down social barriers and promote social mobility. This can lead to a more diverse, inclusive, and equitable society, where everyone has the opportunity to succeed.\\n\\nFinally, higher education is essential for fostering a culture of lifelong learning and personal growth. As people's lives change and circumstances evolve, they need to continually update their skills and knowledge to remain competitive in the job market. Higher education institutions can play a critical role in helping individuals to develop these skills, ensuring that they are equipped to adapt to the rapidly changing world.\\n\\nIn conclusion, higher education is essential for fostering a more informed, productive, and prosperous society. It provides individuals with the knowledge, skills, and expertise necessary to compete in the modern job market, stimulate innovation and creativity, address social and economic inequalities, promote social mobility, and foster a culture of lifelong learning and personal growth. As we look to the future, it is clear that higher education will remain a critical component of a thriving and sustainable society.\\n\\nReferences:\\n\\n* Organisation for Economic Co-operation and Development (OECD). (2019). Education at a Glance 2019: Key Indicators.\\n* United Nations Educational, Scientific and Cultural Organization (UNESCO). (2019). Education for All by 2030.\\n* World Bank. (2020). Education at a Glance 2020.\\n\\nNote: The essay is based on available data and information up to the cut-off date.\", tool_calls=None, raw={'output': tensor([   791,  94100,    315,  35321,  11930,     25,  58530,    287,    279,\n",
       "         55057,    315,    279,   9479,  24367,    271,  88545,   6873,    706,\n",
       "          1317,   1027,  27458,    439,    279,   1401,    311,  80478,    279,\n",
       "          4754,    315,   7931,     11,  34775,     11,    323,  37671,     13,\n",
       "          1102,    374,    264,  82575,    315,   6617,  36017,     11,   8405,\n",
       "          7931,    449,    279,   7512,     11,   6677,     11,    323,  19248,\n",
       "          5995,    311,  12265,    304,    459,  15098,   6485,    323,  83416,\n",
       "          1917,     13,    578,  12939,    315,   5190,   6873,   4250,    387,\n",
       "         83509,    660,     11,    323,    433,    374,   7718,    369,  86644,\n",
       "           264,    810,  16369,     11,  27331,     11,    323,  71407,   8396,\n",
       "           382,   4054,    315,    279,   6156,   8125,   5190,   6873,    374,\n",
       "           779,  16996,    374,    430,    433,   3312,   3153,   7931,    449,\n",
       "           279,   6677,    323,   7512,   5995,    311,  20874,    304,    279,\n",
       "          6617,   2683,   3157,     13,    763,   3432,    596,   3728,   8752,\n",
       "            11,    279,   5410,    315,   5496,    323,   7100,   6650,    527,\n",
       "          6089,  17791,    311,    279,   2237,    315,   6873,    323,   4967,\n",
       "            13,  35321,   6873,  14673,   3493,   4236,    449,   2680,    311,\n",
       "           264,  13057,   1358,    315,  14307,     11,   7620,     11,    323,\n",
       "          3495,  10708,    430,    649,   1520,   1124,   2274,    279,   7512,\n",
       "           323,  19248,   2631,    311,  12265,    304,    872,  12146,   2115,\n",
       "            13,   1789,   2937,     11,   7978,    617,   6982,    430,   7931,\n",
       "           449,   5190,   5990,    315,   6873,   8541,    311,   7380,   5190,\n",
       "         37532,     11,   4774,   2731,   2683,  27949,     11,    323,   3217,\n",
       "         11293,  19542,   7969,    382,  57417,     11,   5190,   6873,  11335,\n",
       "           264,   9200,   3560,    304,  86644,    264,   7829,    315,  19297,\n",
       "           323,  28697,     13,   3296,   8405,   2680,    311,  14713,  48448,\n",
       "          3495,    323,  14584,   5070,     11,  23978,    323,   5190,   6873,\n",
       "         14673,    649,  51077,    279,   4500,    315,    502,   6848,     11,\n",
       "         14645,     11,    323,   3956,     13,   1115,     11,    304,   2543,\n",
       "            11,    649,   3063,    311,   7100,   6650,     11,   2683,   9886,\n",
       "            11,    323,  13241,   5496,  10886,     13,   1789,   3187,     11,\n",
       "          1690,   5961,    430,    617,  29091,  17345,    304,   5190,   6873,\n",
       "           617,  10534,  11295,   7100,   6650,    323,  19297,     11,    439,\n",
       "          3970,    304,    279,   1162,    315,  21181,     11,    902,    706,\n",
       "          3719,    264,  19240,    369,  14713,  48448,   3495,    323,   5557,\n",
       "           382,  88545,   6873,   1101,  11335,    264,  16595,   3560,    304,\n",
       "         28118,   3674,    323,   7100,  93334,     13,   3296,   8405,   2680,\n",
       "           311,   5190,   6873,     11,  17047,    323,  14673,    649,   1520,\n",
       "         14497,    279,  13225,   1990,    279,    305,   4798,    323,    617,\n",
       "         30269,     82,     11,  23391,    430,   5127,    706,    279,   6776,\n",
       "           311,   2274,    279,   7512,    323,   6677,   5995,    311,  16136,\n",
       "          7373,    304,   8396,     13,   1115,     11,    304,   2543,     11,\n",
       "           649,   1520,    311,   8108,  19542,     11,  32305,     11,    323,\n",
       "          3674,  42308,    382,  14364,   9200,  13189,    315,   5190,   6873,\n",
       "           374,   1202,   5845,    311,  31087,   9200,   7422,     11,   3575,\n",
       "         99246,     11,    323,  20632,   7512,     13,   3296,  26921,   4236,\n",
       "           311,   1781,  41440,     11,  13488,   6485,   4819,     11,    323,\n",
       "           990,    304,   7411,     11,  23978,    323,   5190,   6873,  14673,\n",
       "           649,   1520,    311,   2274,    279,   7512,   5995,    311,  22118,\n",
       "           279,   6485,   5435,  13176,   1057,   1917,   3432,     13,   1115,\n",
       "            11,    304,   2543,     11,    649,   3063,    311,  13241,   5597,\n",
       "         28846,     11,   4947,   4500,     11,    323,   4029,  20392,    382,\n",
       "           644,   5369,     11,   5190,   6873,    706,    264,  28254,   5536,\n",
       "           389,   3674,  31139,     13,   3296,   8405,  10708,    369,   7931,\n",
       "           311,  23564,   5190,   6873,     11,  23978,    323,  14673,    649,\n",
       "          1520,    311,   1464,   1523,   3674,  30740,    323,  12192,   3674,\n",
       "         31139,     13,   1115,    649,   3063,    311,    264,    810,  17226,\n",
       "            11,  29408,     11,    323,  77109,   8396,     11,   1405,   5127,\n",
       "           706,    279,   6776,    311,  12265,    382,  24901,     11,   5190,\n",
       "          6873,    374,   7718,    369,  86644,    264,   7829,    315,  51263,\n",
       "          6975,    323,   4443,   6650,     13,   1666,   1274,    596,   6439,\n",
       "          2349,    323,  13463,  38680,     11,    814,   1205,    311,  35611,\n",
       "          2713,    872,   7512,    323,   6677,    311,   7293,  15022,    304,\n",
       "           279,   2683,   3157,     13,  35321,   6873,  14673,    649,   1514,\n",
       "           264,   9200,   3560,    304,  10695,   7931,    311,   2274,   1521,\n",
       "          7512,     11,  23391,    430,    814,    527,  19167,    311,  10737,\n",
       "           311,    279,  19019,  10223,   1917,    382,    644,  17102,     11,\n",
       "          5190,   6873,    374,   7718,    369,  86644,    264,    810,  16369,\n",
       "            11,  27331,     11,    323,  71407,   8396,     13,   1102,   5825,\n",
       "          7931,    449,    279,   6677,     11,   7512,     11,    323,  19248,\n",
       "          5995,    311,  20874,    304,    279,   6617,   2683,   3157,     11,\n",
       "         51077,  19297,    323,  28697,     11,   2686,   3674,    323,   7100,\n",
       "         93334,     11,  12192,   3674,  31139,     11,    323,  31087,    264,\n",
       "          7829,    315,  51263,   6975,    323,   4443,   6650,     13,   1666,\n",
       "           584,   1427,    311,    279,   3938,     11,    433,    374,   2867,\n",
       "           430,   5190,   6873,    690,   7293,    264,   9200,   3777,    315,\n",
       "           264,  53414,    323,  22556,   8396,    382,  32812,   1473,      9,\n",
       "         47843,    369,  23362,   3623,  67334,    323,  11050,    320,  53965,\n",
       "          6620,    570,    320,    679,     24,    570,  11930,    520,    264,\n",
       "          8444,    685,    220,    679,     24,     25,   5422,   2314,  43152,\n",
       "           627,      9,   3723,  19687,  46945,     11,  38130,    323,  41333,\n",
       "         21021,    320,   1899,  71466,    570,    320,    679,     24,    570,\n",
       "         11930,    369,   2052,    555,    220,   9639,     15,    627,      9,\n",
       "          4435,   8715,     13,    320,   2366,     15,    570,  11930,    520,\n",
       "           264,   8444,    685,    220,   2366,     15,    382,   9290,     25,\n",
       "           578,   9071,    374,   3196,    389,   2561,    828,    323,   2038,\n",
       "           709,    311,    279,   4018,  12744,   2457,     13, 128009]), 'completion_kwargs': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "    },\n",
    "]\n",
    "response = model(conversation)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d7d74f-c5e8-4628-93f9-913af1c60bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How many seconds would it take for a leopard at full speed to run through Pont des Arts?</span>                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ VLLMModel - meta-llama/Llama-3.2-1B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow many seconds would it take for a leopard at full speed to run through Pont des Arts?\u001b[0m                        \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m VLLMModel - meta-llama/Llama-3.2-1B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 147.69 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 147.69 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> math</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Speed of the leopard in meters per second</span><span style=\"background-color: #272822\">                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">leopard_speed_mps </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">80</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Distance covered by the leopard</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">distance </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">360</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># meters</span><span style=\"background-color: #272822\">                                                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Calculate the time it would take for the leopard to run through the bridge</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">time_to_run </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> distance </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> leopard_speed_mps</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Convert the time to seconds</span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">time_in_seconds </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> time_to_run </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">*</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3600</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The time it would take for the leopard to run through Pont des Arts is\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, time_in_seconds, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"seconds\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #ed007e; text-decoration-color: #ed007e; background-color: #1e0010\">```</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Final answer:</span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmath\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Speed of the leopard in meters per second\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_mps\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m80\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Distance covered by the leopard\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdistance\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m360\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# meters\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Calculate the time it would take for the leopard to run through the bridge\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdistance\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_mps\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Convert the time to seconds\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtime_in_seconds\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m*\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3600\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe time it would take for the leopard to run through Pont des Arts is\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_in_seconds\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mseconds\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;237;0;126;48;2;30;0;16m`\u001b[0m\u001b[38;2;237;0;126;48;2;30;0;16m`\u001b[0m\u001b[38;2;237;0;126;48;2;30;0;16m`\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mFinal\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34manswer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code parsing failed on line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">16</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> due to: SyntaxError</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">```</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> ^</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error: invalid syntax (&lt;unknown&gt;, line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">16</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode parsing failed on line \u001b[0m\u001b[1;31m16\u001b[0m\u001b[1;31m due to: SyntaxError\u001b[0m\n",
       "\u001b[1;31m```\u001b[0m\n",
       "\u001b[1;31m ^\u001b[0m\n",
       "\u001b[1;31mError: invalid syntax \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m<\u001b[0m\u001b[1;31munknown\u001b[0m\u001b[1;31m>\u001b[0m\u001b[1;31m, line \u001b[0m\u001b[1;31m16\u001b[0m\u001b[1;31m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 225.69 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 225.69 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> math</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Speed of the leopard in meters per second</span><span style=\"background-color: #272822\">                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">leopard_speed_mps </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">80</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Distance covered by the leopard</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">distance </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">360</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># meters</span><span style=\"background-color: #272822\">                                                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Calculate the time it would take for the leopard to run through the bridge</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">time_to_run </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> distance </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> leopard_speed_mps</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Convert the time to seconds</span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">time_in_seconds </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> time_to_run </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">*</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3600</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The time it would take for the leopard to run through Pont des Arts is\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, time_in_seconds, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"seconds\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">135</span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmath\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Speed of the leopard in meters per second\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_mps\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m80\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Distance covered by the leopard\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdistance\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m360\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# meters\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Calculate the time it would take for the leopard to run through the bridge\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdistance\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mleopard_speed_mps\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Convert the time to seconds\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtime_in_seconds\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_to_run\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m*\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3600\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe time it would take for the leopard to run through Pont des Arts is\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtime_in_seconds\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mseconds\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m135\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "The time it would take for the leopard to run through Pont des Arts is 16200.0 seconds\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "The time it would take for the leopard to run through Pont des Arts is 16200.0 seconds\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed at line </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'final_answer = 135'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> due to: InterpreterError: Cannot assign to name </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'final_answer'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">doing this would erase the existing tool!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode execution failed at line \u001b[0m\u001b[1;31m'final_answer = 135'\u001b[0m\u001b[1;31m due to: InterpreterError: Cannot assign to name \u001b[0m\u001b[1;31m'final_answer'\u001b[0m\u001b[1;31m: \u001b[0m\n",
       "\u001b[1;31mdoing this would erase the existing tool!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 246.17 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 246.17 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 63.34 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 63.34 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m CodeAgent(tools\u001b[38;5;241m=\u001b[39m[DuckDuckGoSearchTool()], model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow many seconds would it take for a leopard at full speed to run through Pont des Arts?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2025/smolagents/src/smolagents/agents.py:323\u001b[0m, in \u001b[0;36mMultiStepAgent.run\u001b[0;34m(self, task, stream, reset, images, additional_args)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask, images\u001b[38;5;241m=\u001b[39mimages)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Outputs are returned only at the end as a string. We only look at the last step\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/2025/smolagents/src/smolagents/agents.py:332\u001b[0m, in \u001b[0;36mMultiStepAgent._run\u001b[0;34m(self, task, images)\u001b[0m\n\u001b[1;32m    330\u001b[0m memory_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_memory_step(step_start_time, images)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    334\u001b[0m     memory_step\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/2025/smolagents/src/smolagents/agents.py:352\u001b[0m, in \u001b[0;36mMultiStepAgent._execute_step\u001b[0;34m(self, task, memory_step)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplanning_step(task, is_first_step\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m), step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_rule(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mLogLevel\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m--> 352\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_answer_checks:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_final_answer(final_answer)\n",
      "File \u001b[0;32m~/2025/smolagents/src/smolagents/agents.py:1216\u001b[0m, in \u001b[0;36mCodeAgent.step\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1215\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrammar\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1216\u001b[0m     chat_message: ChatMessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<end_code>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObservation:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m     memory_step\u001b[38;5;241m.\u001b[39mmodel_output_message \u001b[38;5;241m=\u001b[39m chat_message\n\u001b[1;32m   1222\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m chat_message\u001b[38;5;241m.\u001b[39mcontent\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mVLLMModel.__call__\u001b[0;34m(self, messages, stop_sequences, grammar, tools_to_call_from, images, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     completion_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_new_tokens\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m output \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_sequences \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/entrypoints/llm.py:749\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, messages, sampling_params, use_tqdm, lora_request, chat_template, chat_template_content_format, add_generation_prompt, continue_final_message, tools, mm_processor_kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m         prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmm_processor_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mm_processor_kwargs\n\u001b[1;32m    747\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/utils.py:1080\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1076\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1077\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m         )\n\u001b[0;32m-> 1080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/entrypoints/llm.py:470\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    460\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    463\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    464\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    468\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 470\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/entrypoints/llm.py:1377\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1375\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1377\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/engine/llm_engine.py:1391\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1388\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1389\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1391\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/executor/executor_base.py:284\u001b[0m, in \u001b[0;36mDistributedExecutorBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_worker_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_worker_execution_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    281\u001b[0m         async_run_tensor_parallel_workers_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m driver_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver_execute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m driver_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver_outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/executor/mp_distributed_executor.py:144\u001b[0m, in \u001b[0;36mMultiprocessingDistributedExecutor._driver_execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_driver_execute_model\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: Optional[ExecuteModelRequest]\n\u001b[1;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[SamplerOutput]]:\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run execute_model in the driver worker.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    Passing None will cause the driver to stop the model execution\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    loop running in each of the remote workers.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/worker/worker_base.py:420\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    417\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    418\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 420\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/worker/cpu_model_runner.py:654\u001b[0m, in \u001b[0;36mCPUModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps, previous_hidden_states)\u001b[0m\n\u001b[1;32m    649\u001b[0m     execute_model_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    650\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: previous_hidden_states})\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(model_input\u001b[38;5;241m.\u001b[39mattn_metadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config,\n\u001b[1;32m    653\u001b[0m                          model_input\u001b[38;5;241m.\u001b[39mvirtual_engine):\n\u001b[0;32m--> 654\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexecute_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmultimodal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# Compute the logits.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompute_logits(hidden_states,\n\u001b[1;32m    666\u001b[0m                                    model_input\u001b[38;5;241m.\u001b[39msampling_metadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/model_executor/models/llama.py:543\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    536\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 543\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/compilation/decorators.py:172\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# torch.compiler.is_compiling() means we are inside the compilation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# e.g. TPU has the compilation logic in model runner, so we don't\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# need to compile the model inside.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_compile \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling():\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# the first compilation needs to have dynamic shapes marked\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_codes) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/model_executor/models/llama.py:368\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    367\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 368\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    376\u001b[0m     })\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/model_executor/models/llama.py:282\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    281\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 282\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    288\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    289\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/model_executor/models/llama.py:203\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    198\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     attn_metadata: AttentionMetadata,\n\u001b[1;32m    202\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 203\u001b[0m     qkv, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    205\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/model_executor/layers/linear.py:388\u001b[0m, in \u001b[0;36mColumnParallelLinear.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Matrix multiply.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m output_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_output:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# All-gather across the partitions.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     output \u001b[38;5;241m=\u001b[39m tensor_model_parallel_all_gather(output_parallel)\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm-0.7.4.dev53+geb24dc4a.d20250223.cpu-py3.12-linux-x86_64.egg/vllm/model_executor/layers/linear.py:142\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.apply\u001b[0;34m(self, layer, x, bias)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m           layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    139\u001b[0m           x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    140\u001b[0m           bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n",
    "\n",
    "agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5551f71-4d34-43a8-9017-d0d44f686c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

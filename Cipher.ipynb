{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5a6cd10-abd3-4afa-acf5-6910591ef5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "457551b0-48c0-4674-9ce0-a6f7eed9d0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12650"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.comb(25,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f981337-e763-467e-b971-bc48aa504155",
   "metadata": {},
   "outputs": [],
   "source": [
    "cipher = \"\"\"Xoi Doijtooi Tooijto-Ntoit-oo Toooj bot iott boi Giüobooj boi Öotoiooboooi to Jobi 4014 toit 40.000 Nojoioojojoo oot bto Xäoboi noo Ditnottooboo to Xootiobjoob joiobioopt. Dti zoo Jobioioobo iojjoo oi ioob 100.000 Dojojoo ioto. 4044 taootoo ooob otoooj io ntojo Nojoibäoboi btozotooooo. Xoiüpoi btoooi bot Toooj poiotti Iooioobo Tooijtoiootoboi, Loboitottoooo tüi Tjottioootoi oob Uäioooooooo pot iotooo Kooboo toitojjtoit. Noboo to oäobitoo Jobi taooto oto Iotj btoioi Dojojoo oji otooi boi jiaßtoo ntitoojjoo Kiottnoito to Toiooo oo Tooijtoooitt oottiotoo.\n",
    "\"\"\"\n",
    "text = \"\"\"Das Berliner Energie-Start-up Enpal hat seit der Gründung des Unternehmens im Jahr 2017 fast 70.000 Solaranlagen auf die Dächer von Privatkunden in Deutschland geschraubt. Bis zum Jahresende sollen es rund 100.000 Anlagen sein. 2025 könnten noch einmal so viele Solardächer hinzukommen. Darüber hinaus hat Enpal bereits Tausende Energiespeicher, Ladestationen für Elektroautos und Wärmepumpen bei seinen Kunden installiert. Schon im nächsten Jahr könnte ein Teil dieser Anlagen als eines der größten virtuellen Kraftwerke in Europa am Energiemarkt auftreten.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a7824f46-4b74-46a5-9cc1-3991bc0bc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cipher_list = cipher.lower().split(' ')\n",
    "cipher_list = [word.strip('.').strip('\\n') for word in cipher_list ] \n",
    "\n",
    "text_list = text.lower().split(' ')\n",
    "text_list = [word.strip('.').strip('\\n') for word in text_list ] \n",
    "\n",
    "pairs = {(i, k) for i,k in zip(cipher_list, text_list)}\n",
    "rules = {}\n",
    "\n",
    "for pair in pairs:\n",
    "    for c, s in zip(pair[0],pair[1]):\n",
    "        if c in rules:\n",
    "            if s not in rules[c]:\n",
    "                rules[c] += s\n",
    "        else:\n",
    "            rules[c] = s\n",
    "rules['r'] = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "835908f9-00c1-4f6f-8153-d01a472588d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "e\n",
      "f\n",
      "h\n",
      "m\n",
      "q\n",
      "s\n",
      "v\n",
      "w\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "abc = ''.join([chr(ord('a')+i) for i in range(ord('z') - ord('a')+1)])\n",
    "for i in range(ord('z') - ord('a')):\n",
    "    ch = chr(ord('a')+i)\n",
    "    if ch not in rules:\n",
    "        print(ch)\n",
    "        rules[ch] = abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "10c9bfd6-1a5d-4b1b-b800-3e02ae57c162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 'enmaupoc',\n",
       " 't': 'ietfk',\n",
       " 'j': 'lgj',\n",
       " 'd': 'bpa',\n",
       " 'i': 'rst',\n",
       " '-': '-',\n",
       " 'n': 'svw',\n",
       " '4': '275',\n",
       " '0': '0',\n",
       " '1': '1',\n",
       " 'b': 'hd',\n",
       " 'ü': 'ü',\n",
       " 'z': 'z',\n",
       " 'p': 'b',\n",
       " '.': '.',\n",
       " 'ä': 'ä',\n",
       " 'a': 'ö',\n",
       " 'ß': 'ß',\n",
       " 'x': 'd',\n",
       " 'u': 'w',\n",
       " ',': ',',\n",
       " 'g': 'g',\n",
       " 'l': 'l',\n",
       " 'k': 'k',\n",
       " 'ö': 'u',\n",
       " 'r': 'r',\n",
       " 'c': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'e': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'f': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'h': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'm': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'q': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 's': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'v': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'w': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'y': 'abcdefghijklmnopqrstuvwxyz'}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5ab58f3a-15eb-4af5-868e-e2ddc1a9e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dictionary = None\n",
    "path_to_dict = '../aime-analysis/wortliste.txt'\n",
    "with open(path_to_dict, 'r') as f:\n",
    "    raw_dictionary = f.readlines()\n",
    "\n",
    "raw_dictionary = [word.strip() for word in raw_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b927efd1-d9d3-48e6-aaa4-18fbee37b372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nächstem', 'nächsten', 'nächstens']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_dictionary_word(cipher, raw_dictionary=raw_dictionary, rules=rules):\n",
    "    lower_cipher = cipher.lower().strip()\n",
    "    restricted_dict = [word.lower() for word in raw_dictionary if len(lower_cipher) <= len(word) <= len(lower_cipher)+1]\n",
    "    for i, letter in enumerate(lower_cipher):\n",
    "        tmp_dict = []\n",
    "        allowed = rules[letter]\n",
    "        for word in restricted_dict:\n",
    "            if word[i] in allowed and word not in tmp_dict:\n",
    "                tmp_dict += [word] \n",
    "        restricted_dict = tmp_dict\n",
    "    return restricted_dict\n",
    "\n",
    "find_dictionary_word('oäobitoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "37e1d6c1-44b3-466f-8a95-b5bd9cd10ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7289856"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_valid_german_word(word, abbr = False):\n",
    "    \"\"\"\n",
    "    Validates if a word follows German letter combination rules.\n",
    "    Returns True if the word is valid, False if it contains impossible combinations.\n",
    "    \"\"\"\n",
    "    vowels = 'aeiouäöüy'\n",
    "    consonants = 'bcdfghjklmnpqrstvwxz'\n",
    "    word = word.lower()\n",
    "    \n",
    "    # 1. \"dt\" at the beginning of a word\n",
    "    if word.startswith('dt'):\n",
    "        return False\n",
    "    \n",
    "    # 2. \"ck\" at the beginning of a word\n",
    "    if word.startswith('ck'):\n",
    "        return False\n",
    "    \n",
    "    # 3. \"tz\" at the end without a vowel before it\n",
    "    if word.endswith('tz') and len(word) > 2:\n",
    "        if word[-3] not in vowels:\n",
    "            return False\n",
    "    \n",
    "    # 4. \"pf\", \"sch\", \"st\", \"sp\" at the end without a vowel before them\n",
    "    end_groups = ['pf', 'st', 'sp']\n",
    "    if any(word.endswith(group) and len(word) > len(group) and word[-(len(group)+1)] not in vowels for group in end_groups):\n",
    "        return False\n",
    "    if word.endswith('sch') and len(word) > 3 and word[-4] not in vowels:\n",
    "        return False\n",
    "    \n",
    "    # 5. \"ä\", \"ö\", \"ü\" followed immediately by another vowel\n",
    "    for i in range(len(word) - 1):\n",
    "        if word[i] in 'äöü' and word[i+1] in vowels:\n",
    "            return False\n",
    "    \n",
    "    # 7. \"q\" without a following \"u\"\n",
    "    for i in range(len(word) - 1):\n",
    "        if word[i] == 'q' and word[i+1] != 'u':\n",
    "            return False\n",
    "    if word.endswith('q'):\n",
    "        return False\n",
    "\n",
    "    if len(word) > 3 and all([c in vowels for c in word]) and not abbr:\n",
    "        return False\n",
    "\n",
    "    if all([c in consonants for c in word]) and not abbr:\n",
    "        return False\n",
    "    \n",
    "    # 12. Triple consonants without a hyphen\n",
    "    for i in range(len(word) - 2):\n",
    "        if (word[i] in consonants and \n",
    "            word[i+1] in consonants and \n",
    "            word[i+2] in consonants and\n",
    "            word[i] == word[i+1] == word[i+2]):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def produce_variants(word, rules=rules):\n",
    "    result = []\n",
    "    vectors = [list(rules[c]) for c in word]\n",
    "    for prod in it.product(*vectors):\n",
    "        word = ''.join(prod)\n",
    "        if is_valid_german_word(word):\n",
    "            yield word\n",
    "\n",
    "\n",
    "len([var for var in produce_variants('ronojo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f072e02-c3f3-481c-81a7-294f889a7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dbmdz/german-gpt2\"\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0fb52bce-ac3c-4403-b98f-288cae7d7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dictionary variants\n",
      " |  Werdersa 2.315834125024193e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Werdersa'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_model_variants(phrase, word, predictions, model=model, tokenizer=tokenizer, n_tokens = 5, cut=0.90):\n",
    "    next_token_logits = predictions[0, -1, :]\n",
    "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    top_k_tokens = torch.topk(next_token_probs, k=n_tokens)  # Get top 10 possible next tokens\n",
    "    # Decode the token IDs to words\n",
    "    next_words = [tokenizer.decode([token_id], skip_special_tokens=True) for token_id in top_k_tokens.indices]\n",
    "    \n",
    "    # Print the next possible words and their probabilities\n",
    "    for var_word, var_prob in zip(next_words, top_k_tokens.values):\n",
    "        var_word = var_word.strip()\n",
    "        if len(var_word) == len(word) and var_prob.item() > cut:\n",
    "            print(f\"Word: {var_word} Probability: {var_prob.item()}\")\n",
    "            return  var_word, var_prob\n",
    "    return None, -1\n",
    "\n",
    "def decipher(text, model=model, tokenizer=tokenizer, rules=rules, prior='\\n'):\n",
    "    result = prior\n",
    "\n",
    "    raw_text_list = text.split(' ')\n",
    "    text_list = []\n",
    "    punctuations = [',', '.', ';', '\\n', '?', '!', '\"']\n",
    "    for word in raw_text_list:\n",
    "        if word[-1] in punctuations:\n",
    "            for punct in punctuations:\n",
    "                if word.endswith(punct):\n",
    "                    text_list += [word.strip(punct), punct, ' ']\n",
    "        elif '-' in word:\n",
    "            text_list += word.split('-')  \n",
    "        else:\n",
    "            text_list += [word]    \n",
    "    \n",
    "    for word in text_list:\n",
    "        if not word or word == ' ':\n",
    "            continue\n",
    "        if word in punctuations:\n",
    "            result += word\n",
    "            continue\n",
    "\n",
    "        inputs = tokenizer(result, return_tensors='pt')\n",
    "        # Get model outputs (logits)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to probabilities using softmax\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        variants = find_dictionary_word(word.lower())\n",
    "        print(f'{len(variants)} dictionary variants')\n",
    "        if len(variants) == 0:\n",
    "            variants = produce_variants(word.lower())\n",
    "            \n",
    "        best_var, max_prob = check_model_variants(result, word, logits)\n",
    "        #best_var = None\n",
    "        if best_var is None:\n",
    "            max_prob = 0.0\n",
    "            for variant in variants:\n",
    "                tmp = ' '\n",
    "                if word.istitle():\n",
    "                    tmp += variant.capitalize()\n",
    "                else:\n",
    "                    tmp += variant\n",
    "                #next_token_id = tokenizer.encode(tmp)[0]  # Token ID for the next token \"example\"\n",
    "                #probability = probabilities[0, -1, next_token_id].item()\n",
    "                next_token_ids = tokenizer.encode(tmp)\n",
    "                probability = 1.0\n",
    "                for token_id in next_token_ids:\n",
    "                    # Assuming we are calculating the probability of the next tokens after the input text\n",
    "                    probability *= probabilities[0, -1, token_id].item()\n",
    "    \n",
    "                if probability > max_prob:\n",
    "                    max_prob = probability\n",
    "                    best_var = tmp\n",
    "        print(result.strip()[len(prior)-1:], '|', best_var, max_prob)\n",
    "        if result[-1] != ' ' and best_var[0] != ' ':\n",
    "            result += ' '\n",
    "        result += best_var\n",
    "    return result\n",
    "\n",
    "decipher('Uoiboiio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936105e-2a47-4833-9b3d-1e0314ee9cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080afa7-0088-4004-a6fa-4a6ef3882b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decipher('Noboo to oäobitoo Jobi taooto oto Iotj btoioi Dojojoo oji otooi boi jiaßtoo ntitoojjoo Kiottnoito to Toiooo oo Tooijtoooitt oottiotoo', \n",
    "         prior='Aus hunderttausend Solardächern wird ein Großkraftwerk.\\nDas Berliner Energieunternehmen Enpal und die Münchner Softwarefirma Entrix gründen ein Joint Venture / Künstliche Intelligenz soll virtuelles Kraftwerk optimieren.\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "748f41ab-a5ff-4bcf-9ca6-7f46385dde97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 dictionary variants\n",
      " |  Doch 7.34875413854752e-08\n",
      "74 dictionary variants\n",
      "\n",
      " Doch |  was 0.0174531489610672\n",
      "31 dictionary variants\n",
      "\n",
      " Doch was |  passiert 0.0320647694170475\n",
      "85 dictionary variants\n",
      "Word: wenn Probability: 0.9662197828292847\n",
      "\n",
      " Doch was passiert, | wenn tensor(0.9662)\n",
      "64 dictionary variants\n",
      "\n",
      " Doch was passiert, wenn |  die 0.15111319720745087\n",
      "5 dictionary variants\n",
      "\n",
      " Doch was passiert, wenn die |  Künstliche 3.7553607311358754e-12\n",
      "0 dictionary variants\n",
      "Word: Intelligenz Probability: 0.9925306439399719\n",
      "\n",
      " Doch was passiert, wenn die Künstliche | Intelligenz tensor(0.9925)\n",
      "76 dictionary variants\n",
      "\n",
      " Doch was passiert, wenn die Künstliche Intelligenz |  mit 0.010966898873448372\n",
      "8 dictionary variants\n",
      "\n",
      " Doch was passiert, wenn die Künstliche Intelligenz mit |  ihrem 0.04068536311388016\n",
      "0 dictionary variants\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecipher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXoob noi ooiitoit, nooo bto Küoitjtobo Iotojjtjooz ott tbioo Uoiboiiojoo tüi boi ntitoojjo Kiottnoit boob otoooj booopoojtojt oob boi Djjoittbooi oot boo Tooijtoooitt Ntioo noitoott, boo boi Koobo noo Toooj otjootjtob tüi bto Uoiobooiobtoo pooattjt?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDas Berliner Energieunternehmen Enpal und die Münchner Softwarefirma Entrix gründen ein Joint Venture / Künstliche Intelligenz soll virtuelles Kraftwerk optimieren.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[221], line 64\u001b[0m, in \u001b[0;36mdecipher\u001b[0;34m(text, model, tokenizer, rules, prior)\u001b[0m\n\u001b[1;32m     61\u001b[0m     tmp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m variant\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#next_token_id = tokenizer.encode(tmp)[0]  # Token ID for the next token \"example\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#probability = probabilities[0, -1, next_token_id].item()\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m next_token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m next_token_ids:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Assuming we are calculating the probability of the next tokens after the input text\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2607\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2608\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2631\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2642\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2644\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3063\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3054\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3055\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3056\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3060\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3061\u001b[0m )\n\u001b[0;32m-> 3063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:126\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m )\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:613\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    611\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    612\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 613\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:116\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m )\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:539\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 539\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    551\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    553\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    563\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decipher('Xoob noi ooiitoit, nooo bto Küoitjtobo Iotojjtjooz ott tbioo Uoiboiiojoo tüi boi ntitoojjo Kiottnoit boob otoooj booopoojtojt oob boi Djjoittbooi oot boo Tooijtoooitt Ntioo noitoott, boo boi Koobo noo Toooj otjootjtob tüi bto Uoiobooiobtoo pooattjt?',\n",
    "        prior='Das Berliner Energieunternehmen Enpal und die Münchner Softwarefirma Entrix gründen ein Joint Venture / Künstliche Intelligenz soll virtuelles Kraftwerk optimieren.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fc38b-472e-48e5-87bc-fe570248357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coded_text = \"\"\"Xoi Doijtooi Tooijto-Ntoit-oo Toooj bot iott boi Giüobooj boi Öotoiooboooi to Jobi 4014 toit 40.000 Nojoioojojoo oot bto Xäoboi noo Ditnottooboo to Xootiobjoob joiobioopt. Dti zoo Jobioioobo iojjoo oi ioob 100.000 Dojojoo ioto. 4044 taootoo ooob otoooj io ntojo Nojoibäoboi btozotooooo. Xoiüpoi btoooi bot Toooj poiotti Iooioobo Tooijtoiootoboi, Loboitottoooo tüi Tjottioootoi oob Uäioooooooo pot iotooo Kooboo toitojjtoit. Noboo to oäobitoo Jobi taooto oto Iotj btoioi Dojojoo oji otooi boi jiaßtoo ntitoojjoo Kiottnoito to Toiooo oo Tooijtoooitt oottiotoo.\n",
    "\"Uooo ooo Ttozojbooibojto jojoo bto Ntioooiotitoino oottotoit, tooo ooo jooz ntojo Xtojo joi otobt oooboo, notj boi Ntiooooitt otobt tüi iojobo Giaßoo joboobt tit\", iojt Doojooto Voijo, Lottoi boi Diobottootntotjooj pot Toooj, zo boo Djäooo tüi boi ntitoojjo Kiottnoit. \"Voo tooo otoo otozojoo Ktjonottitoobo otobt oo Tooijtoooitt boobojo\", iojt oi üpoi boizott ooob tobjoobo Uottotoioojioajjtobtottoo tüi bto Dojojoo boi Kooboo. Öo ott boo oot boo Nojoibäoboio oizoojtoo Ntioo zo boobojo oob ooob bto Nootoboi tüi bto Uottotoiooj oo Ntiooooitt zo ootzoo, oüiitoo bto Ttozojoojojoo zo otooi Ejotto zoiooooojoiobjoiioo noiboo, bto oo Tooijtoooitt nto oto Kiottnoit to Gtjonottooßitop oottittt. \"Xoi tit oi, noi nti ootoi otooo ntitoojjoo Kiottnoit 4.0 noiitoboo\", iojt Voijo. Öob joooo boi ntjj Toooj to otooo Jotot Uootoio ott boo Vüoobooi Tooijtoiottnoioootoioobooo Totitx ooob to btoioo Jobi oot boo Uoj pitojoo. Xotüi tonoittoit Toooj otooo znotitojjtjoo Vtjjtoooopotioj, ott boo itob bto Doijtooi bto Vobibott oo boo Goootoiobottiootoioobooo Ejoxo itoboio. Doiotti to noijoojoooo Jobi botto itob Toooj oo Totitx pototjtjt.\n",
    "\"\"\"\n",
    "prior = \"Das Berliner Energieunternehmen Enpal und die Münchner Softwarefirma Entrix gründen ein Joint Venture / Künstliche Intelligenz soll virtuelles Kraftwerk optimieren.\\n\"\n",
    "\n",
    "decipher(coded_text, prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3718f125-730f-4ee9-acd6-efeaae83e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 dictionary variants\n",
      ". |  Die 0.056865014135837555\n",
      "2 dictionary variants\n",
      ". Die |  Revolution 0.0015954561531543732\n",
      "68 dictionary variants\n",
      ". Die Revolution |  der 0.19477318227291107\n",
      "1 dictionary variants\n",
      ". Die Revolution der |  Arbeitswelt 0.00209758966229856\n",
      "11 dictionary variants\n",
      ". Die Revolution der Arbeitswelt |  durch 0.0067873080261051655\n",
      "5 dictionary variants\n",
      ". Die Revolution der Arbeitswelt durch |  Künstliche 5.383833255503088e-09\n",
      "0 dictionary variants\n",
      "Word: Intelligenz Probability: 0.9957321286201477\n",
      ". Die Revolution der Arbeitswelt durch Künstliche | Intelligenz tensor(0.9957)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[224], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m prior \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mKI-Revolution im Büro erfordert Umdenken in der IT\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mAktuelle IT-Infrastrukturen behindern KI-Produktivitätsgewinne. Während Unternehmen große Hoffnungen in generative KI setzen, lähmen veraltete digitale Arbeitsplätze die Entfaltung des vollen Potentials.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m coded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mXto Ronojottoo boi Dipottinojt boiob Küoitjtobo Iotojjtjooz tooot toi Ntootoo - otobt otno notj bto Iooboojojto zo nootj jotitoo taooto, iooboio notj bto Vooioboo zo joojioo joiooo oob bto II-Iotioitiottoioo boi ootitoo Öotoioobooo otobt poiott tit. Zo btoioo Tijopoti tooot otoo ottoojjo jjopojo Ntobto ootoi 600 II-Eübioojitiättoo, bto otoo potiäobtjtobo Xtitiooooz zntioboo boo boboo Tinoitoojoo oo KI-joitützto Diobotttnttätiitotjoioojoo oob boi Roojttät to boo Öotoioobooo zotjt.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdecipher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoded_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[221], line 42\u001b[0m, in \u001b[0;36mdecipher\u001b[0;34m(text, model, tokenizer, rules, prior)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Get model outputs (logits)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Convert logits to probabilities using softmax\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1084\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1082\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1084\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prior = \"\"\"KI-Revolution im Büro erfordert Umdenken in der IT\n",
    "Aktuelle IT-Infrastrukturen behindern KI-Produktivitätsgewinne. Während Unternehmen große Hoffnungen in generative KI setzen, lähmen veraltete digitale Arbeitsplätze die Entfaltung des vollen Potentials.\"\"\"\n",
    "\n",
    "coded_text = \"\"\"Xto Ronojottoo boi Dipottinojt boiob Küoitjtobo Iotojjtjooz tooot toi Ntootoo - otobt otno notj bto Iooboojojto zo nootj jotitoo taooto, iooboio notj bto Vooioboo zo joojioo joiooo oob bto II-Iotioitiottoioo boi ootitoo Öotoioobooo otobt poiott tit. Zo btoioo Tijopoti tooot otoo ottoojjo jjopojo Ntobto ootoi 600 II-Eübioojitiättoo, bto otoo potiäobtjtobo Xtitiooooz zntioboo boo boboo Tinoitoojoo oo KI-joitützto Diobotttnttätiitotjoioojoo oob boi Roojttät to boo Öotoioobooo zotjt.\n",
    "\"\"\"\n",
    "decipher(coded_text, prior=prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10ec7e-85bb-40a9-8a90-022433fefe94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
